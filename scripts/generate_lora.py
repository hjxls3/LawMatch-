import torch
from post_process_output import post_process_output
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# è·¯å¾„é…ç½®
base_model_path = "../models/DeepSeek-R1-Distill-Qwen-1.5B"
lora_adapter_path = "../models/lora_adapter2.0"

# å…¨å±€å˜é‡
tokenizer = None
model = None

def load_model():
    """
    æ¨¡å‹åŠ è½½ï¼Œä»…åœ¨é¦–æ¬¡è°ƒç”¨æ—¶æ‰§è¡Œã€‚
    """
    global tokenizer, model

    if tokenizer is None or model is None:
        print("\nğŸ”„ æ­£åœ¨é¦–æ¬¡åŠ è½½æ¨¡å‹...")
        # åŠ è½½ Tokenizer
        tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)

        # åŠ è½½åŸºç¡€æ¨¡å‹å¹¶åˆå¹¶ LoRA adapter
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_path, trust_remote_code=True, torch_dtype=torch.float16, device_map="auto"
        )
        model = PeftModel.from_pretrained(base_model, lora_adapter_path)
        model = model.merge_and_unload()
        model.eval()
    else:
        print("\nâœ… æ¨¡å‹å·²åŠ è½½ï¼Œç›´æ¥ä½¿ç”¨...")


def generate_lora(prompt):
    """
    æ ¹æ®è¾“å…¥çš„ promptï¼Œç”Ÿæˆç½ªåä¸æ³•æ¡ä¿¡æ¯ã€‚
    è¿”å›æ ¼å¼åŒ–åçš„è¾“å‡ºã€‚
    """
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²åŠ è½½
    load_model()

    # ç¼–ç è¾“å…¥
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    # ç”Ÿæˆè¾“å‡º
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=200,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
        )

    # è§£ç è¾“å‡º
    output_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    #print(output_text)

    # è§£æç½ªåä¸æ³•æ¡ä¿¡æ¯
    accusations, articles = post_process_output(output_text, prompt)

    # æ ¼å¼åŒ–è¾“å‡º
    accusation_str = "ï¼Œ".join(accusations) if accusations else "æ— "
    articles_str = "ï¼Œ".join([f"ã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‹ç¬¬{article}æ¡" for article in articles]) if articles else "æ— "

    formatted_output = f"ç½ªåï¼š{accusation_str}\næ³•æ¡ï¼š{articles_str}"

    # è¿”å›æ ¼å¼åŒ–è¾“å‡ºå’Œè§£æåçš„åˆ—è¡¨ï¼Œæ–¹ä¾¿åç»­å¤„ç†
    return formatted_output, accusations, articles


if __name__ == "__main__":
    # ç¤ºä¾‹æµ‹è¯•
    example_prompt = (
        "è¯·æ ¹æ®ä»¥ä¸‹æ¡ˆæƒ…åˆ¤æ–­ç½ªåä¸é€‚ç”¨æ³•æ¡ï¼Œå¹¶æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š\n"
        "ç½ªåï¼šXXXç½ª\n"
        "æ³•æ¡ï¼šã€Šä¸­åäººæ°‘å…±å’Œå›½åˆ‘æ³•ã€‹ç¬¬XXXæ¡\n"
        "è¯·ä¸è¦é‡å¤æ¡ˆæƒ…å†…å®¹ï¼Œç›´æ¥å¼€å§‹å›ç­”ã€‚\n\n"
        "æ¡ˆæƒ…æè¿°å¦‚ä¸‹ï¼šå¼ ä¸‰æŒåˆ€æŠ¢åŠ«ä¾¿åˆ©åº—ï¼Œè‡´äººé‡ä¼¤ã€‚"
    )

    result, accusations, articles = generate_lora(example_prompt)

    print("\nç”Ÿæˆç»“æœï¼š")
    print(result)
    print("\nç½ªååˆ—è¡¨ï¼š", accusations)
    print("æ³•æ¡åˆ—è¡¨ï¼š", articles)
